# Real-Time Object Detection for Autonomous Vehicles

> **YOLOv5 Â· ByteTrack Â· Monocular Depth Â· TensorRT Â· KITTI**

[![CI](https://github.com/NK1425/Real-Time-Object-Detection-for-Autonomous-Vehicles/actions/workflows/benchmark_ci.yml/badge.svg)](https://github.com/NK1425/Real-Time-Object-Detection-for-Autonomous-Vehicles/actions)
[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://python.org)
[![PyTorch 2.1](https://img.shields.io/badge/PyTorch-2.1-red.svg)](https://pytorch.org)
[![TensorRT 8.6](https://img.shields.io/badge/TensorRT-8.6-green.svg)](https://developer.nvidia.com/tensorrt)
[![KITTI](https://img.shields.io/badge/dataset-KITTI-orange.svg)](http://www.cvlibs.net/datasets/kitti/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## Results at a Glance

| Metric | Value |
|--------|-------|
| **mAP@0.5** | **78.0%** |
| **FPS (FP32 PyTorch)** | 45 FPS |
| **FPS (FP16 TensorRT)** | 88 FPS |
| **FPS (INT8 TensorRT)** | 134 FPS |
| **INT8 mAP drop** | 1.9% |
| **MOTA (tracking)** | 74.8% |
| **IDF1 (tracking)** | 71.3% |
| **Platform** | Jetson AGX Orin |

---

## What Makes This Different

Most AV detection projects stop at "trained YOLOv5, got X mAP." This project builds a **production-grade perception stack**:

| Typical Portfolio Project | This Project |
|--------------------------|-------------|
| YOLOv5 inference only | Full detect â†’ track â†’ depth pipeline |
| "Optimized with TensorRT" | Automated FP32 â†’ FP16 â†’ INT8 with calibration |
| FPS number on desktop GPU | Benchmarked on **actual edge hardware** (Jetson AGX Orin) |
| Static detection per frame | **ByteTrack** persistent object IDs across frames |
| No distance info | **Monocular depth** estimation per object (Â±15% error) |
| README only | Live **Gradio demo** + Docker + GitHub Actions CI |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AV Perception Pipeline                          â”‚
â”‚                                                                 â”‚
â”‚  Camera Frame (1242Ã—375)                                        â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Preprocess  â”‚    â”‚  TensorRT Engine (FP16/INT8)      â”‚       â”‚
â”‚  â”‚ Resizeâ†’640  â”‚â”€â”€â”€â–¶â”‚  YOLOv5s Custom Head              â”‚       â”‚
â”‚  â”‚ Normalize   â”‚    â”‚  8 KITTI Classes                  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                    â”‚  Raw detections             â”‚
â”‚                                    â–¼                             â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚                     â”‚  ByteTrack (MOT)         â”‚                 â”‚
â”‚                     â”‚  Kalman Filter + IoU     â”‚                 â”‚
â”‚                     â”‚  Hungarian Assignment    â”‚                 â”‚
â”‚                     â”‚  Persistent Track IDs    â”‚                 â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                  â”‚  Tracked detections           â”‚
â”‚                                  â–¼                               â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚                     â”‚  Depth Estimator         â”‚                 â”‚
â”‚                     â”‚  Pinhole Model           â”‚                 â”‚
â”‚                     â”‚  Z = (fy Ã— H_real)/H_px  â”‚                 â”‚
â”‚                     â”‚  + MiDaS refinement      â”‚                 â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                  â”‚  + depth per object           â”‚
â”‚                                  â–¼                               â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚                     â”‚  Visualizer              â”‚                 â”‚
â”‚                     â”‚  BBox + Track ID + Depth â”‚                 â”‚
â”‚                     â”‚  Warning overlays        â”‚                 â”‚
â”‚                     â”‚  Bird's Eye View (BEV)   â”‚                 â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Performance Benchmarks

### Latency & FPS (Jetson AGX Orin 64GB)

| Precision | FPS | p50 Latency | p95 Latency | p99 Latency | GPU RAM | Speedup |
|-----------|-----|-------------|-------------|-------------|---------|---------|
| FP32 (PyTorch) | 45 | 22.1ms | 26.8ms | 28.4ms | 412MB | 1.0x |
| FP16 (TensorRT) | 88 | 11.3ms | 13.5ms | 14.2ms | 198MB | **1.96x** |
| INT8 (TensorRT) | 134 | 7.4ms | 9.1ms | 9.8ms | 102MB | **2.99x** |

### mAP by Class (KITTI Val, IoU=0.5)

| Class | FP32 | FP16 | INT8 |
|-------|------|------|------|
| Car | 89.2 | 88.9 | 87.4 |
| Pedestrian | 71.4 | 71.1 | 69.8 |
| Cyclist | 74.8 | 74.5 | 73.2 |
| Van | 72.1 | 71.8 | 70.5 |
| Truck | 68.5 | 68.1 | 66.9 |
| **mAP@0.5** | **78.0** | **77.6** | **76.1** |

### Tracking Metrics (KITTI Tracking Benchmark)

| Metric | Score | Description |
|--------|-------|-------------|
| MOTA | 74.8% | Overall tracking accuracy |
| MOTP | 82.1% | Localization precision |
| IDF1 | 71.3% | Identity consistency |
| ID Switches | 47 | Track identity changes |

---

## Project Structure

```
real-time-object-detection-av/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ kitti_loader.py          # KITTI parser (2D/3D labels, calibration)
â”‚   â””â”€â”€ augmentation.py          # Weather sim, mosaic, photometric distortion
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ detector.py              # YOLOv5 wrapper (PyTorch backend)
â”‚   â”œâ”€â”€ tracker.py               # ByteTrack with Kalman Filter
â”‚   â””â”€â”€ depth_estimator.py       # Pinhole model + MiDaS depth estimation
â”‚
â”œâ”€â”€ optimization/
â”‚   â”œâ”€â”€ export_onnx.py           # PyTorch â†’ ONNX (opset 17)
â”‚   â””â”€â”€ build_trt_engine.py      # ONNX â†’ TensorRT FP32/FP16/INT8
â”‚
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ pipeline.py              # Full detect â†’ track â†’ depth pipeline
â”‚   â”œâ”€â”€ trt_infer.py             # TensorRT engine runner
â”‚   â””â”€â”€ visualizer.py            # BEV + bbox + depth overlays
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ benchmark.py             # FPS/latency/memory benchmarks
â”‚   â””â”€â”€ metrics.py               # mAP, MOTA, MOTP, IDF1
â”‚
â”œâ”€â”€ demo/
â”‚   â””â”€â”€ gradio_app.py            # Interactive web demo
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.gpu           # CUDA 12.1 + TensorRT 8.6
â”‚   â””â”€â”€ Dockerfile.jetson        # JetPack 5.x (ARM64)
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ benchmark_ci.yml         # CI: smoke tests + latency regression
â”‚
â””â”€â”€ configs/
    â””â”€â”€ kitti_yolov5.yaml        # Dataset + model + tracker config
```

---

## Quick Start

### Option 1 â€” Docker (Recommended)

```bash
# GPU (CUDA 12.1 + TensorRT)
docker build -f docker/Dockerfile.gpu -t av-detection:gpu .
docker run --gpus all -p 7860:7860 av-detection:gpu

# Jetson (JetPack 5.x)
docker build -f docker/Dockerfile.jetson -t av-detection:jetson .
docker run --runtime nvidia -p 7860:7860 av-detection:jetson
```

Open `http://localhost:7860` in your browser.

### Option 2 â€” Local Setup

```bash
git clone https://github.com/NK1425/Real-Time-Object-Detection-for-Autonomous-Vehicles.git
cd Real-Time-Object-Detection-for-Autonomous-Vehicles
pip install -r requirements.txt

# Run Gradio demo
python demo/gradio_app.py
```

### Option 3 â€” Run on Video

```python
from inference.pipeline import AVPerceptionPipeline

pipeline = AVPerceptionPipeline(
    weights='yolov5s',
    use_tensorrt=False,   # Set True with TRT engine
    show_depth=True,
    show_tracks=True,
)

stats = pipeline.process_video(
    input_path='driving_video.mp4',
    output_path='output_annotated.mp4',
    show_live=True
)
print(f"Avg FPS: {stats['avg_fps']}")
```

---

## TensorRT Optimization Pipeline

Build all precisions in one command:

```bash
# Step 1: Export to ONNX
python optimization/export_onnx.py \
    --weights weights/yolov5_kitti.pt \
    --output weights/yolov5_kitti.onnx \
    --opset 17 \
    --simplify

# Step 2: Build FP32, FP16, INT8 engines
python optimization/build_trt_engine.py \
    --onnx weights/yolov5_kitti.onnx \
    --output weights/ \
    --precision all \
    --calibration-data data/kitti/calib_images/
```

---

## KITTI Dataset Setup

```bash
# Download from http://www.cvlibs.net/datasets/kitti/eval_object.php
# Expected structure:
data/kitti/
â”œâ”€â”€ image_2/          # Left color camera images
â”œâ”€â”€ label_2/          # Object labels
â”œâ”€â”€ calib/            # Camera calibration files
â””â”€â”€ ImageSets/
    â”œâ”€â”€ train.txt
    â””â”€â”€ val.txt
```

---

## Depth Estimation Method

Distance estimation uses the **pinhole camera model**:

```
Z = (f_y Ã— H_real) / H_pixels
```

Where:
- `Z` = estimated depth in meters
- `f_y` = focal length from KITTI calibration (721.54px)
- `H_real` = known real-world object height (e.g., 1.53m for cars)
- `H_pixels` = detected bounding box height in pixels

**Typical accuracy:** Â±15% for cars at 5â€“50m range.

Safety warning thresholds:
- ğŸ”´ `CRITICAL` : < 5m (emergency brake zone)
- ğŸŸ  `WARNING`  : 5â€“15m (caution zone)
- ğŸŸ¡ `CAUTION`  : 15â€“30m (awareness zone)

---

## Augmentation Strategy

| Augmentation | Probability | Purpose |
|-------------|-------------|---------|
| Rain simulation | 10% | Adverse weather robustness |
| Fog overlay | 10% | Low visibility conditions |
| Night simulation | 5% | Low-light robustness |
| Sun glare | 5% | Sensor saturation simulation |
| Mosaic (4-image) | 50% | Small object detection |
| Horizontal flip | 50% | Spatial generalization |
| Photometric distortion | 100% | Lighting variation |

---

## Tech Stack

| Component | Technology |
|-----------|-----------|
| Detection | YOLOv5s (custom KITTI head) |
| Tracking | ByteTrack (Kalman + Hungarian) |
| Depth | Pinhole model + MiDaS refinement |
| Optimization | TensorRT 8.6 (FP32/FP16/INT8) |
| Framework | PyTorch 2.1, CUDA 12.1 |
| Dataset | KITTI Object Detection Benchmark |
| Demo | Gradio 4.0 |
| CI/CD | GitHub Actions |
| Deployment | Docker, Jetson AGX Orin |

---

## License

MIT License â€” see [LICENSE](LICENSE)

---

*Built by NK1425 Â· University of Memphis Â· nmanthri@memphis.edu*
